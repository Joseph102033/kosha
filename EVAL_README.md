# 법령 추천 정확도 평가 시스템

## 📋 개요

Safe OPS Studio의 법령 추천 시스템은 골든 데이터셋 기반의 정량적 평가를 통해 지속적으로 개선됩니다.

**평가 페이지**: `/admin/eval`

---

## 🎯 평가 메트릭 설명

### 1. Precision@K (정밀도)
**정의**: 상위 K개 추천 중 정답 법령의 비율

**계산 방식**:
```
Precision@K = (상위 K개 중 정답 개수) / K
```

**예시**:
- 추천: [A, B, C, D, E]
- 정답: [A, D, F]
- Precision@3 = 1/3 = 33.3% (상위 3개 중 A만 정답)
- Precision@5 = 2/5 = 40.0% (상위 5개 중 A, D가 정답)

**해석**:
- **높을수록 좋음** (추천의 신뢰도)
- 사용자가 상위 몇 개만 볼 때 유용한 지표
- **권장 기준**: Precision@3 ≥ 50%, Precision@5 ≥ 40%

---

### 2. Recall@K (재현율)
**정의**: 전체 정답 중 상위 K개에서 찾은 비율

**계산 방식**:
```
Recall@K = (상위 K개 중 정답 개수) / (전체 정답 개수)
```

**예시**:
- 추천: [A, B, C, D, E]
- 정답: [A, D, F]
- Recall@3 = 1/3 = 33.3% (3개 정답 중 A만 찾음)
- Recall@5 = 2/3 = 66.7% (3개 정답 중 A, D 찾음)

**해석**:
- **높을수록 좋음** (정답 커버리지)
- 중요한 법령을 놓치지 않았는지 확인
- **권장 기준**: Recall@5 ≥ 60%

---

### 3. F1@K (조화평균)
**정의**: Precision@K와 Recall@K의 조화평균

**계산 방식**:
```
F1@K = 2 × (Precision@K × Recall@K) / (Precision@K + Recall@K)
```

**해석**:
- Precision과 Recall의 균형을 평가
- **권장 기준**: F1@5 ≥ 50%

---

### 4. MRR (Mean Reciprocal Rank)
**정의**: 첫 번째 정답이 나타난 순위의 역수 평균

**계산 방식**:
```
MRR = 1 / (첫 정답의 순위)
```

**예시**:
- 추천: [A, B, C, D, E]
- 정답: [C, F]
- MRR = 1/3 = 33.3% (C가 3번째 위치)

**해석**:
- **높을수록 좋음** (첫 정답이 빨리 나타남)
- 사용자가 첫 정답을 빨리 찾을 수 있는지 평가
- **권장 기준**: MRR ≥ 0.5 (평균 2위 이내)

---

### 5. NDCG@K (Normalized Discounted Cumulative Gain)
**정의**: 순서를 고려한 추천 품질 (상위일수록 높은 가중치)

**계산 방식**:
```
DCG@K = Σ (rel_i / log2(i+1))  (i=1 to K)
NDCG@K = DCG@K / IDCG@K
```

**예시**:
- 추천: [A, B, C] (A, C가 정답)
- DCG = 1/log2(2) + 0 + 1/log2(4) = 1.0 + 0 + 0.5 = 1.5
- IDCG = 1/log2(2) + 1/log2(3) = 1.0 + 0.63 = 1.63
- NDCG = 1.5 / 1.63 = 92%

**해석**:
- **1.0 = 완벽한 순서** (모든 정답이 최상위)
- 순서가 중요한 추천 시스템에 적합
- **권장 기준**: NDCG@5 ≥ 0.7

---

## 📊 정확도 리포트 읽는 법

### 종합 점수 해석

| 메트릭 | 우수 | 양호 | 개선 필요 |
|--------|------|------|-----------|
| **Precision@3** | ≥ 60% | 40-60% | < 40% |
| **Precision@5** | ≥ 50% | 30-50% | < 30% |
| **Recall@5** | ≥ 70% | 50-70% | < 50% |
| **MRR** | ≥ 0.6 | 0.4-0.6 | < 0.4 |
| **NDCG@5** | ≥ 0.8 | 0.6-0.8 | < 0.6 |

---

### 시나리오별 해석

#### 시나리오 1: 높은 Precision, 낮은 Recall
```
Precision@5: 80%
Recall@5: 30%
```
**의미**: 추천된 법령은 정확하지만, 중요한 법령을 많이 놓침
**개선 방법**:
- 키워드 매칭 범위 확대
- 유사 법령 검색 로직 추가
- 골든 데이터셋의 누락 법령 재검토

---

#### 시나리오 2: 낮은 Precision, 높은 Recall
```
Precision@5: 30%
Recall@5: 80%
```
**의미**: 많은 법령을 추천하지만 관련 없는 법령이 많음
**개선 방법**:
- 키워드 매칭 정확도 향상
- 불필요한 룰 제거
- 우선순위 로직 개선

---

#### 시나리오 3: 낮은 MRR, 높은 Recall
```
MRR: 0.3
Recall@5: 70%
```
**의미**: 정답을 찾지만 순서가 나쁨
**개선 방법**:
- 법령 순위 알고리즘 개선
- 중요도 가중치 조정
- 키워드 신뢰도 스코어링 추가

---

#### 시나리오 4: 낮은 NDCG, 높은 Recall
```
NDCG@5: 0.5
Recall@5: 75%
```
**의미**: 정답을 많이 찾지만 순서가 뒤죽박죽
**개선 방법**:
- 법령 중요도 재정의
- 순위 학습 알고리즘 도입 검토
- 사용자 피드백 반영

---

## 🔬 평가 프로세스

### 1. 평가 실행

```bash
# 개발 서버 실행
cd apps/web
npm run dev

# 브라우저에서 접속
http://localhost:3000/admin/eval
```

**실행 순서**:
1. "▶️ 평가 시작" 버튼 클릭
2. 시스템이 골든 데이터셋 20건에 대해 법령 추천 실행
3. 추천 결과와 정답 비교
4. 메트릭 계산 및 결과 표시
5. 자동으로 이력에 저장 (localStorage)

---

### 2. 결과 분석

**메트릭 카드**:
- 주요 4개 지표를 한눈에 확인
- 색상 코딩: 초록(우수), 노랑(양호), 빨강(개선 필요)

**상세 표**:
- 8개 메트릭의 정확한 수치 확인
- P@3, R@3, P@5, R@5, F1@3, F1@5, MRR, NDCG@5

---

### 3. 추세 분석

**추세 그래프**:
- 룰셋 수정 후 점수 변화 확인
- 여러 버전 비교
- SVG 차트로 시각화

**이력 테이블**:
- 버전별 전체 메트릭 비교
- 날짜별 성능 추이 확인

---

### 4. 개별 케이스 분석

**케이스 상세보기**:
1. 골든 데이터셋에서 케이스 클릭
2. 사고 설명 및 원인 확인
3. 정답 법령 vs 추천 법령 비교
4. 매칭/누락 법령 분석
5. 개별 메트릭 확인

---

## 🛠️ 룰셋 개선 워크플로우

### 단계별 가이드

#### 1단계: 현재 성능 파악
```bash
1. /admin/eval 접속
2. "▶️ 평가 시작" 실행
3. 현재 점수 확인 (기준선)
```

**체크포인트**:
- [ ] Precision@5 ≥ 40%?
- [ ] Recall@5 ≥ 60%?
- [ ] MRR ≥ 0.5?

---

#### 2단계: 문제 케이스 분석
```bash
1. 골든 데이터셋에서 낮은 점수 케이스 클릭
2. "누락된 법령" 섹션 확인
3. 왜 매칭되지 않았는지 원인 파악
   - 키워드 부재?
   - 룰 우선순위 낮음?
   - 법령 자체가 DB에 없음?
```

---

#### 3단계: 룰셋 수정
```typescript
// apps/workers/src/law/search.ts 또는 관련 파일

// 예시 1: 키워드 추가
{
  keyword: "추락",
  law_title: "산업안전보건기준에 관한 규칙 제42조",
  // 기존 키워드에 "고소작업대", "사다리" 추가
}

// 예시 2: 새 룰 추가
{
  keyword: "고소작업대",
  law_title: "산업안전보건기준에 관한 규칙 제157조",
}
```

---

#### 4단계: 재평가
```bash
1. 코드 수정 후 저장
2. /admin/eval에서 "▶️ 평가 시작" 재실행
3. 이전 버전과 점수 비교
4. 추세 그래프에서 변화 확인
```

**성공 기준**:
- [ ] Precision 유지 또는 증가
- [ ] Recall 증가
- [ ] 특정 케이스의 매칭 개선

---

#### 5단계: 버전 기록
```markdown
# 변경 이력 (notes.md 또는 별도 파일)

## v0.2 (2025-10-17)
- 추락 사고 관련 키워드 확장 ("고소작업대", "사다리", "개구부")
- 신규 법령 5건 추가
- **결과**: Recall@5 55% → 68% (↑13%p)

## v0.1 (2025-10-16)
- 초기 룰셋 (50개 키워드)
- **결과**: Precision@5 42%, Recall@5 55%
```

---

## 📈 벤치마크 목표

### MVP 목표 (v1.0)
- **Precision@5**: ≥ 50%
- **Recall@5**: ≥ 70%
- **MRR**: ≥ 0.5
- **NDCG@5**: ≥ 0.7

### 장기 목표 (v2.0)
- **Precision@5**: ≥ 70%
- **Recall@5**: ≥ 85%
- **MRR**: ≥ 0.7
- **NDCG@5**: ≥ 0.85

---

## 🧪 골든 데이터셋 관리

### 데이터셋 구조
**파일**: `apps/web/fixtures/golden-dataset.json`

```json
{
  "version": "1.0",
  "test_cases": [
    {
      "id": "CASE-001",
      "description": "비계 작업 중 추락사고",
      "incident_type": "Fall",
      "incident_cause": "3층 비계 작업 중 안전난간 미설치...",
      "keywords": ["추락", "비계", "안전난간"],
      "relevant_laws": [
        "산업안전보건기준에 관한 규칙 제42조",
        "산업안전보건기준에 관한 규칙 제56조"
      ]
    }
  ]
}
```

---

### 데이터셋 업데이트 가이드

#### 신규 케이스 추가
```json
{
  "id": "CASE-021",
  "description": "[간결한 사고 설명]",
  "incident_type": "Fall|Chemical Spill|Fire|Explosion|Equipment Failure|Other",
  "incident_cause": "[상세 원인 - 실제 OPS 입력과 유사하게]",
  "keywords": ["키워드1", "키워드2"],
  "relevant_laws": [
    "정답 법령 1",
    "정답 법령 2"
  ]
}
```

**주의사항**:
- **익명화**: 회사명, 인명, 구체적 주소 제거
- **PII 제거**: 개인정보 일절 포함 금지
- **정답 법령**: 전문가 검토 필수
- **대표성**: 다양한 재해 유형 커버

---

#### 정답 법령 검증
```bash
1. 산업안전보건공단 공식 사이트 확인
   - https://www.kosha.or.kr

2. 법령 DB에서 조문 확인
   - https://www.law.go.kr

3. 전문가 리뷰 (필수)
   - 산업안전 담당자
   - 법무팀
```

---

## 🔍 트러블슈팅

### 문제 1: 평가 실행 시 모든 케이스가 0점
**원인**: Workers API 엔드포인트 오류
**해결**:
```bash
# Workers 서버 상태 확인
curl https://safe-ops-studio-workers.yosep102033.workers.dev/health

# 로컬 개발 시
wrangler dev
```

---

### 문제 2: 특정 케이스만 계속 0점
**원인**: 골든 데이터셋의 정답 법령이 DB에 없음
**해결**:
```sql
-- D1 DB에서 법령 확인
SELECT * FROM law_rules WHERE law_title LIKE '%제42조%';

-- 없으면 추가
INSERT INTO law_rules (keyword, law_title, url)
VALUES ('추락', '산업안전보건기준에 관한 규칙 제42조', 'https://...');
```

---

### 문제 3: Precision은 높은데 Recall이 낮음
**원인**: 보수적인 매칭 (키워드가 너무 구체적)
**해결**:
```typescript
// 키워드 일반화
// Before: "강관비계 추락"
// After: "비계", "추락" (분리)

// 유사어 추가
// "추락" + "낙하" + "떨어짐"
```

---

### 문제 4: 차트가 표시되지 않음
**원인**: 평가 이력이 2개 미만
**해결**:
- 평가를 2회 이상 실행하여 이력 누적
- 또는 localStorage에서 `eval_history` 직접 추가 (테스트용)

---

## 📚 참고 자료

### Information Retrieval 기초
- **Precision vs Recall**: https://en.wikipedia.org/wiki/Precision_and_recall
- **MRR**: https://en.wikipedia.org/wiki/Mean_reciprocal_rank
- **NDCG**: https://en.wikipedia.org/wiki/Discounted_cumulative_gain

### 추천 시스템 평가
- Microsoft Research: "Evaluation Measures for Recommender Systems"
- Google Research: "Measuring Recommendation Relevance"

---

## 🤝 기여 가이드

### 골든 데이터셋 개선 제안
1. 새로운 재해 유형 추가
2. 기존 케이스의 정답 법령 검증
3. 엣지 케이스 추가 (경계 사례)

### 메트릭 추가 제안
- MAP (Mean Average Precision)
- Coverage (법령 커버리지)
- Diversity (추천 다양성)

---

**Last Updated**: 2025-10-16
**Version**: 1.0
**Maintainer**: Safe OPS Studio Team
